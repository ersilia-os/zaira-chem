[EXPERIMENTS]
# Calling this parameter file will run a fine_tuning experiment.
mode = fine_tuning
# 1000 SMILES will be sampled for each saved model (see "period" below).
n_sample = 1000
# SMILES will be sampled with a temperature parameter of 0.7
temp = 0.70

[PROCESSING]
# 80% of the data are used for training, 20% for validation
split = 0.8
# Minimum length of a SMILES that will be kept from the 
# pretraining and fine-tuning data.
min_len = 1
# Cf above, but maximum length.
max_len = 140

[AUGMENTATION]
# Level of data augmentation to apply each of the space.
source_space = 10
# Note that we do not apply data augmentation to the 
# target space as we use it only to be compared to the 
# generated molecules and the source_space.
target_space = 0
fine_tuning = 10

[MODEL]
# Number of epochs.
epochs = 40
# Learning rate.
lr = 0.0001
# Number of neurons and layers,
# here two layers; first one with 1024 units, the second with 256.
neurons = [1024,256]
# Dropout value to apply atfer each layer defined above.
# Must have the same number of values as in neurons.
dropouts = [0.40, 0.40]
# Define if a layer has to be frozen.
# If False, the layer is frozen (i.e. not trainable).
# Must have the same number of values as in neurons.
trainables = [False, True]
# If the validation loss does not decrease during 3 epochs,
# the learning rate is decerased by "factor", as defined below,
# but the the learning rate will not be decreased lower than
# "min_lr", as defined below.
# See the Keras documentation for more details.
patience_lr = 3
factor = 0.5
min_lr = 0.00005
# How often a model is saved. Here, every 10 epochs. 
# It means that the sampling of new SMILES will be done at those epochs. 
period = 10
# Batch size. You can try to play with this parameter, but it should not
# be higher than the number of molecules in your transfer learning set,
# including the data augmentation.
batch_size = 1
# See the Keras documentation.
n_workers = 1

[DATA]
# Dataset used for pretraining.
source_space = chembl24_cleaned_unique_canon.txt
# Dataset from which the molecules were taken from for the
# publication. Your run will be plotted with those molecules by default.
# You might want to update this if you do not work with natural products.
target_space = MEGx_Release_180901_All_4557_length_filtered_wo_sugar.txt

[FINETUNING]
# Path to the pretrained model to be used for fine-tuning in
# your experiment(s).
path_model = ../models/c24_augmentationx10_minlen1_maxlen140.h5
